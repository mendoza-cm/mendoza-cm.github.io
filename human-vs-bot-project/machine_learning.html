<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Machine Learning</title>
    <style>
        body {
            
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
            max-width: 900px;
            margin: 0 auto;
            line-height: 1.6;
        }
        }
        h1 {
            color: #8f0e0a;
            font-size: 2rem;
            margin-top: 2rem;
        }
        h2 {
            color: #c7408d;
            font-size: 1.4rem;
            margin-top: 1.5rem;
        }
        h3 {
            font-size: 1.2rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        img {
      display: block;
      margin: 2rem auto;
      max-width: 100%;
      height: auto;
    }
        .fraction {
            display: inline-block;
            text-align: center;
            vertical-align: middle;
        }
        .fraction .top {
            display: block;
            border-bottom: 1px solid #000;
            padding: 0 0.2em;
        }
        .fraction .bottom {
            display: block;
            padding: 0 0.2em;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>

    This is the third part of the Human-vs-Bot project. The project begins by simulating data uniquely identifed by id and session id with response times, distinguishable
    usieng timestamps, follow different distributions. Part two developes basic logistic regression models.
    
    
    <section> 
        <h1>Machine Learning</h1>
        <p>
            Although logistic regression is a machine learning method, itâ€™s often introduced in the context of traditional statistics, leading many to overlook its role in classification tasks. 
            In this section, we explore two additional supervised learning approaches: Support Vector Machines and Random Forests.</p>

            
        
        <h2>Support Vector Machines (SVM)</h2>
        <p>
            SVMs calculate the maximal-margin hyperplane that best separates classes in feature space. By choosing a non-linear kernel (e.g. radial or polynomial), they can learn complex decision boundaries. 
            Although SVMs are inherently binary classifiers, they can be extended to multiclass problems via schemes like one-vs-rest (OvR) or one-vs-one (OvO), where multiple SVM models are trained 
            and their outputs combined either by selecting the highest decision-function score or by majority vote. In this project, we compared SVM performance (using linear, radial, and polynomial 
            kernels) to logistic regression, evaluating accuracy.</p>

        <h2>Random Forests (RF)</h2>
        <p>
            Random forests are ensembles of decision trees that reduce overfitting by averaging predictions across many trees, each trained on a bootstrap sample and a random subset of features. 
            This approach naturally captures nonlinear relationships and variable interactions. Because random forests use bagging, you can also estimate performance directly on the training set 
            via out-of-bag (OOB) samples.

            Random Forests is not as sensitive as SVM with respect to scaling predictors and not including those with multi-collinearity. We compare RF models with two predictors vs all possible 
            predictors, but we would not recommend doing this in practice: throw in the entire kitchen sink every time with random forests.
        </p>
    </section>

    <section>
        <h1>Results</h1>
        We developed a total of eight SVM and two RF models. Since SVM is sensitive to multicollinear data, we started off with using the two
        predictors that produced good results for logistic regression; 5 variables were included in the first RF model:
            <ul>
                <li>rate</li>
                <li>sd_inter_response</li>
                <li>duration</li>
                <li>mean_inter_response</li>
                <li>responses_per_session</li>
                
            </ul>
       <p style="text-align: center;">
             <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/classifier_comparison.png" alt="ML Results" width="600"
                 width="400" 
                 height="500">
            </p>
    </section>
    
</body>
</html>
