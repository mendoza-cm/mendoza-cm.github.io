<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning</title>
    <style>
        body {
            
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
            max-width: 900px;
            margin: 0 auto;
            line-height: 1.6;
        }
        
        h1 {
            color: #8f0e0a;
            font-size: 2rem;
            margin-top: 2rem;
        }
        h2 {
            color: #c7408d;
            font-size: 1.4rem;
            margin-top: 1.5rem;
        }
        h3 {
            font-size: 1.2rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        img {
      display: block;
      margin: 2rem auto;
      max-width: 100%;
      height: auto;
    }
        .fraction {
            display: inline-block;
            text-align: center;
            vertical-align: middle;
        }
        .fraction .top {
            display: block;
            border-bottom: 1px solid #000;
            padding: 0 0.2em;
        }
        .fraction .bottom {
            display: block;
            padding: 0 0.2em;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div style="text-align: left; margin-bottom: 2rem;">
    <h1 style="font-size: 2.5rem; color: #444;">Human-vs-Bot</h1>
  </div>

    This is the third installment of the Human-vs-Bot project. The project begins by simulating response behavior, generating data uniquely identified by user 
    and session IDs. Response times are timestamped and drawn from distinct distributions to reflect plausible differences between human and bot activity. In part 
    two, we developed baseline logistic regression models to evaluate the separability of these groups.
    
    
    <section> 
        <h1>Machine Learning</h1>
        <p>
            Although logistic regression is a machine learning method, itâ€™s often introduced in the context of traditional statistics, leading many to overlook its role in classification tasks. 
            In this section, we explore two additional supervised learning approaches: Support Vector Machines and Random Forests.</p>

            
        
        <h2>Support Vector Machines (SVM)</h2>
        <p>
            SVMs calculate the maximal-margin hyperplane that best separates classes in feature space. By choosing a non-linear kernel (e.g. radial or polynomial), they can learn complex decision boundaries. 
            Although SVMs are inherently binary classifiers, they can be extended to multiclass problems via schemes like one-vs-rest (OvR) or one-vs-one (OvO), where multiple SVM models are trained 
            and their outputs combined either by selecting the highest decision-function score or by majority vote. In this project, we compared SVM performance (using linear, radial, and polynomial 
            kernels) to logistic regression, evaluating accuracy.</p>

        <h2>Random Forests (RF)</h2>
        <p>
            Random forests are ensembles of decision trees that reduce overfitting by averaging predictions across many trees, each trained on a bootstrap sample and a random subset of features. 
            This approach naturally captures nonlinear relationships and variable interactions. Because random forests use bagging, you can also estimate performance directly on the training set 
            via out-of-bag (OOB) samples.
</p>
<p>
            Compared to SVMs, Random Forests are less sensitive to feature scaling and multicollinearity. We evaluated RF models using both a reduced set of predictors and an expanded set. 
            While feature selection was used here for illustration, Random Forests are often robust enough to handle a larger number of inputs without strict preprocessing.
        </p>
    </section>

    <section>
        <h1>Results</h1>
    <p>
      We developed eight SVM models and two Random Forest models. Since SVMs are sensitive to multicollinearity, we initially used the two predictors that performed well in logistic regression. The first Random Forest model included five predictors:
    </p>
    <ul>
      <li>rate</li>
      <li>sd_inter_response</li>
      <li>duration</li>
      <li>mean_inter_response</li>
      <li>responses_per_session</li>
    </ul>
    <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/classifier_comparisonUpdated.png" alt="Classifier Comparison Chart">
  </section>
    </section>
    
</body>
</html>
