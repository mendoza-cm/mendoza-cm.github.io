<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Machine Learning</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
        }
        h1 {
            color: #8f0e0a;
            font-size: 2rem;
        }
        h2 {
            color: #c7408d;
            font-size: 1.4rem;
        }
        h3 {
            font-size: 1.2rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            list-style: none;
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        .fraction {
            display: inline-block;
            text-align: center;
            vertical-align: middle;
        }
        .fraction .top {
            display: block;
            border-bottom: 1px solid #000;
            padding: 0 0.2em;
        }
        .fraction .bottom {
            display: block;
            padding: 0 0.2em;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    
    <section> 
        <h1>Machine Learning</h1>
        <p>
            Although logistic regression models fall under the umbrella of Machine Learning, many people don't realize it because they're often taught in a traditional statistics context. 
            However, they are widely used in supervised classification tasksâ€”just like the models below.</p>
        
        <h2>Support Vector Machines (SVM)</h2>
        <p>
            SVMs calculate the maximal-margin hyperplane that best separates classes in feature space. By choosing a non-linear kernel (e.g. radial or polynomial), they can learn complex decision boundaries. 
            Although SVMs are inherently binary classifiers, they can be extended to multiclass problems via schemes like one-vs-rest (OvR) or one-vs-one (OvO), where multiple SVM models are trained 
            and their outputs combined either by selecting the highest decision-function score or by majority vote. In this project, we compared SVM performance (using linear, radial, and polynomial 
            kernels) to logistic regression, evaluating accuracy.</p>

        <h2>Random Forests (RF)</h2>
        <p>
            Random forests are ensembles of decision trees that reduce overfitting by averaging predictions across many trees, each trained on a bootstrap sample and a random subset of features. 
            This approach naturally captures nonlinear relationships and variable interactions. Because random forests use bagging, you can also estimate performance directly on the training set 
            via out-of-bag (OOB) samples.
        </p>
    </section>

    <section>
        <h1>Results</h1>
       <p style="text-align: center;">
             <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/classifier_comparison.png" alt="ML Results" width="600"
                 width="400" 
                 height="500">
            </p>
    </section>
    
</body>
</html>
