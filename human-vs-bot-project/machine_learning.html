<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning</title>
    <style>
        body {
            
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
            max-width: 900px;
            margin: 0 auto;
            line-height: 1.6;
        }
        
        h1 {
            color: #8f0e0a;
            font-size: 2rem;
            margin-top: 2rem;
        }
        h2 {
            color: #c7408d;
            font-size: 1.4rem;
            margin-top: 1.5rem;
        }
        h3 {
            font-size: 1.2rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        img {
      display: block;
      margin: 2rem auto;
      max-width: 100%;
      height: auto;
    }
        .fraction {
            display: inline-block;
            text-align: center;
            vertical-align: middle;
        }
        .fraction .top {
            display: block;
            border-bottom: 1px solid #000;
            padding: 0 0.2em;
        }
        .fraction .bottom {
            display: block;
            padding: 0 0.2em;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <div style="text-align: left; margin-bottom: 2rem;">
    <h1 style="font-size: 2.5rem; color: #444;">Human-vs-Bot</h1>
  </div>


    <p>
        This is the third installment of the Human-vs-Bot project. We began by simulating response behavior and generating timestamped data using distinct time 
        distributions to reflect plausible differences between human and bot activity. In part two, we used logistic regression to explore how separable the two 
        groups were. Here, we extend our modeling by applying Support Vector Machines and Random Forests to the same task.
</p>

    
    
    <section> 
        <h1>Machine Learning</h1>
        <p>
            Although logistic regression is a machine learning method, itâ€™s often introduced in the context of traditional statistics, leading many to overlook its role in classification tasks. 
            In this section, we explore two additional supervised learning approaches: Support Vector Machines and Random Forests.</p>

            
        
        <h2>Support Vector Machines (SVM)</h2>
        <p>
            SVMs calculate the maximal-margin hyperplane that best separates classes in feature space. By choosing a non-linear kernel (e.g. radial or polynomial), they can learn complex decision boundaries. 
            Although SVMs are inherently binary classifiers, they can be extended to multiclass problems via schemes like one-vs-rest (OvR) or one-vs-one (OvO), where multiple SVM models are trained 
            and their outputs combined either by selecting the highest decision-function score or by majority vote. In this project, we compared SVM performance (using linear, radial, and polynomial 
            kernels) to logistic regression, evaluating accuracy.</p>

        <h2>Random Forests (RF)</h2>
        <p>
            Random forests are ensembles of decision trees that reduce overfitting by averaging predictions across many trees, each trained on a bootstrap sample and a random subset of features. 
            This approach naturally captures nonlinear relationships and variable interactions. Because random forests use bagging, you can also estimate performance directly on the training set 
            via out-of-bag (OOB) samples.
</p>
<p>
            Compared to SVMs, Random Forests are less sensitive to feature scaling and multicollinearity. We evaluated RF models using both a reduced set of predictors and an expanded set. 
            While feature selection was used here for illustration, Random Forests are often robust enough to handle a larger number of inputs without strict preprocessing.
        </p>
    </section>

    <section>
        <h1>Results</h1>
    <p>
    We developed eight SVM models and two Random Forest models. Since SVMs can be sensitive to multicollinearity, we began by using the two predictors that had performed well in 
        logistic regression. The first Random Forest model included five predictors:
</p>
<ul>
    <li><code>rate</code></li>
    <li><code>sd_inter_response</code></li>
    <li><code>duration</code></li>
    <li><code>mean_inter_response</code></li>
    <li><code>responses_per_session</code></li>
</ul>

    <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/classifier_comparisonUpdated.png" alt="Classifier Comparison Chart">
  </section>
    </section>
<p style="text-align: center; font-style: italic; color: #666;">
    Figure: Classifier performance by accuracy and AUC across models using both reduced and full feature sets.
</p>

    
</body>
</html>
