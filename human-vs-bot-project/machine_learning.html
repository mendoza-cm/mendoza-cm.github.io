<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Logistic Regression</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
        }
        h1 {
            color: #8f0e0a;
        }
        h2 {
            color: #c7408d;
            margin-top: 1.5rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            list-style: none;
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        .fraction {
            display: inline-block;
            text-align: center;
            vertical-align: middle;
        }
        .fraction .top {
            display: block;
            border-bottom: 1px solid #000;
            padding: 0 0.2em;
        }
        .fraction .bottom {
            display: block;
            padding: 0 0.2em;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    
    <section> 
        <h1>Machine Learning</h1>
        <p>Although logistic regression models fall under the umbrella of Machine Learning, many people don't realize it because they're often taught in a traditional statistics context. However, they are widely used in supervised classification tasksâ€”just like the models below.</p>
        
        <h2>Support Vector Machines (SVM)</h2>
        <p>SVMs work by finding the hyperplane that best separates classes. With non-linear kernels, they can handle complex decision boundaries. In this project, we compared performance to logistic regression using accuracy and AUC.</p>

        <h2>Random Forests (RF)</h2>
        <p>Random Forests are ensembles of decision trees that reduce overfitting by averaging predictions across many trees. They can capture nonlinear relationships and variable interactions well. Feature importance scores were also extracted.</p>
    </section>

    <section>
        <h1>Results</h1>
        <!-- You can continue developing this section -->
    </section>
    
</body>
</html>
