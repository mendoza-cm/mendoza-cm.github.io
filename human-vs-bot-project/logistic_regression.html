<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Logistic Regression</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
        }
        h1 {
            color: #8f0e0a;
            font-size: 2rem;
        }
        h2 {
            color: #c7408d;
            font-size: 1.4rem;
        }
        h3 {
            font-size: 1.2rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            list-style: none;
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        .fraction {
  display: inline-block;
  text-align: center;
  vertical-align: middle;
}
.fraction .top {
  display: block;
  border-bottom: 1px solid #000;
  padding: 0 0.2em;
}
.fraction .bottom {
  display: block;
  padding: 0 0.2em;
}
    </style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
    
       
    
    <section> 
        <h1>Logistic Regression</h1>
             <p>Logistic regression models the log-odds of an event as a linear combination of variables; the odds can be re-written in terms of probabilities \( p \) : 
<div class="eq-c">
  <p>\[
Y = \log\left(\frac{p}{1 - p}\right) = X\beta
\]</p>
</div>
        
    Here, \(X\) is an \(n \times (\nu + 1)\) matrix, where \( n \) is the number of observations and \(\nu\) is the number of predictors (excluding the intercept term).
    We use \(\nu\) to avoid confusion with \( p \), which denotes the predicted probability in logistic regression. In this model, \(\beta\) is a \((\nu + 1) \times 1\) vector of coefficients,
    and \(Y\) is an \(n \times 1 \) vector of logit scores. 
    </p>
    <p>
    Taking the exponential of both sides, we isolate the probability \( p \):
    </p>
        <div class="eq-c">
          <p>\[
          p = \frac{e^{X\beta}}{1 + e^{X\beta}}
          \]</p>
        </div>
    <p>
    This is known as the sigmoid or logistic function. It maps the linear combination of inputs into a probability between 0 and 1.
</p>

    <p>
    Logistic regression is often used as a classification tool. To use logistic regression as a classifier, we must choose a threshold or cut-point to convert predicted probabilities 
    into class labels. A common choice is 0.5, meaning that cases with predicted probabilities greater than 0.5 are classified as positive (e.g., bots), and those with probabilities 
    less than or equal to 0.5 as negative (e.g., humans). In our Human-vs-Bot project, we define bots as the positive class (isBot == 1), so a logit score above 0 (i.e., \( p \) > 0.5)
    would indicate a likely bot.
    </p>

    <p>
    Deciding where to place a cut-point is not always easy. In many cases, it is imperative to look at several cut-points and their corresponding discrimination metrics and predictive values.
    Discrimination metrics help us assess a model’s ability to distinguish between the two classes, independent of any specific cut-point. These include the true positive rate (sensitivity), 
    false positive rate, and the area under the receiver operating characteristic (ROC) curve (AUC). AUC measures the probability that a randomly selected positive case (e.g., bot) is assigned 
    a higher logit score (predicted probability) than a randomly selected negative case (e.g., human).
    </p>

    <p>
    Predictive performance requires selecting a specific cut-point. The positive predictive value (PPV) is the proportion of predicted positives that are true positives (i.e., predicted bots 
    that actually are bots), while the negative predictive value (NPV) is the proportion of predicted negatives that are truly negative (i.e., predicted humans that are actually humans). These 
    metrics depend on both the cut-point and the underlying class distribution.
    </p>

    <p></p>
    A decision table is a practical tool for evaluating potential cut-points in logistic regression. It lists a range of thresholds (e.g., 0.0, 0.1, 0.2, ..., 1.0 for probabilities--though logit 
    scores may also be used) alongside key performance metrics such as True Positive Rate (TPR or sensitivity), False Positive Rate (FPR), Positive Predictive Value (PPV), and Negative Predictive 
    Value (NPV).

    For instance, a cut-point of 0.0 classifies all cases as bots (isBot = 1), which maximizes TPR but also FPR, resulting in low PPV due to many false positives. In contrast, a cut-point of 1.0 
        classifies no cases as bots, yielding TPR = 0 and FPR = 0, with high NPV but no positive predictions. Intermediate thresholds like 0.5 aim to balance these metrics. The optimal cut-point 
        ultimately depends on the project’s priorities—such as whether it's more important to detect bots or minimize false positives.
    </p>
    </section>
    <section>
        <h1>Results</h1>

        <h2>ROC Curve</h2>
            <p>
                The AUC was found to be 0.94
                <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/roc_curve.png" alt="ROC Curve" width="600">
            </p>
        <h2>Decision Table</h2>



    </section>
    

</body>
</html>
