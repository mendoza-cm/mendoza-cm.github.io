<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Logistic Regression</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
        }
        h1 {
            color: #8f0e0a;
            font-size: 2rem;
        }
        h2 {
            color: #c7408d;
            font-size: 1.4rem;
        }
        h3 {
            font-size: 1.2rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            list-style: none;
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        .fraction {
  display: inline-block;
  text-align: center;
  vertical-align: middle;
}
.fraction .top {
  display: block;
  border-bottom: 1px solid #000;
  padding: 0 0.2em;
}
.fraction .bottom {
  display: block;
  padding: 0 0.2em;
}
        table, th, td {
  border:1px solid black;
}
    </style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
    
       
    
    <section> 
        <h1>Logistic Regression</h1>
             <p>Logistic regression models the log-odds of an event as a linear combination of variables; the odds can be re-written in terms of probabilities \( p \) : 
<div class="eq-c">
  <p>\[
Y = \log\left(\frac{p}{1 - p}\right) = X\beta
\]</p>
</div>
        
    Here, \(X\) is an \(n \times (\nu + 1)\) matrix, where \( n \) is the number of observations and \(\nu\) is the number of predictors (excluding the intercept term).
    We use \(\nu\) to avoid confusion with \( p \), which denotes the predicted probability in logistic regression. In this model, \(\beta\) is a \((\nu + 1) \times 1\) vector of coefficients,
    and \(Y\) is an \(n \times 1 \) vector of logit scores. 
    </p>
    <p>
    Taking the exponential of both sides, we isolate the probability \( p \):
    </p>
        <div class="eq-c">
          <p>\[
          p = \frac{e^{X\beta}}{1 + e^{X\beta}}
          \]</p>
        </div>
    <p>
    This is known as the sigmoid or logistic function. It maps the linear combination of inputs into a probability between 0 and 1.
</p>

    <p>
    Logistic regression is often used as a classification tool. To use logistic regression as a classifier, we must choose a threshold or cut-point to convert predicted probabilities 
    into class labels. A common choice is 0.5, meaning that cases with predicted probabilities greater than 0.5 are classified as positive (e.g., bots), and those with probabilities 
    less than or equal to 0.5 as negative (e.g., humans). In our Human-vs-Bot project, we define bots as the positive class (isBot == 1), so a logit score above 0 (i.e., \( p \) > 0.5)
    would indicate a likely bot.
    </p>

    <p>
    Deciding where to place a cut-point is not always easy. In many cases, it is imperative to look at several cut-points and their corresponding discrimination metrics and predictive values.
    Discrimination metrics help us assess a model’s ability to distinguish between the two classes, independent of any specific cut-point. These include the true positive rate (sensitivity), 
    false positive rate, and the area under the receiver operating characteristic (ROC) curve (AUC). AUC measures the probability that a randomly selected positive case (e.g., bot) is assigned 
    a higher logit score (predicted probability) than a randomly selected negative case (e.g., human).

    </p>

    <p>
        Accuracy is often considered with respect to how well a model works; it summarizes discrimination measures of true positives and true negatives by adding them together and
        dividing by the total number of cases classified. That is, it is a measure of the proportion of cases that were correctly identified by the model. It is not a measure
        of how well a model predicts, however. Accuracy reflects overall correctness but doesn't reveal whether the model systematically misclassifies one group.
    </p>

    <p>
    Predictive performance requires selecting a specific cut-point. The positive predictive value (PPV) is the proportion of predicted positives that are true positives (i.e., predicted bots 
    that actually are bots), while the negative predictive value (NPV) is the proportion of predicted negatives that are truly negative (i.e., predicted humans that are actually humans). These 
    metrics depend on both the cut-point and the underlying class distribution.
    </p>

    <p>
    A decision table is a practical tool for evaluating potential cut-points in logistic regression. It lists a range of thresholds (e.g., 0.0, 0.1, 0.2, ..., 1.0 for probabilities--though logit 
    scores may also be used) alongside key performance metrics such as True Positive Rate (TPR or sensitivity), False Positive Rate (FPR), Positive Predictive Value (PPV), and Negative Predictive 
    Value (NPV).

    For instance, a cut-point of 0.0 classifies all cases as bots (isBot = 1), which maximizes TPR but also FPR, resulting in low PPV due to many false positives. In contrast, a cut-point of 1.0 
    classifies no cases as bots, yielding TPR = 0 and FPR = 0, with high NPV but no positive predictions. Intermediate thresholds like 0.5 aim to balance these metrics. The optimal cut-point 
    ultimately depends on the project’s priorities—such as whether it's more important to detect bots or minimize false positives.
    </p>
    </section>
    <section>
        <h1>Results</h1>

        <h2>Model Selection</h2>
        We fit two logistic models for our data: one using only the predictor, response rate (<em>rate</em>), and another using both, response rate and 
        standard deviation of inter-response times (<em>rate</em> & <em>sd_inter_response</em>).

        <pre>
            Call:
            glm(formula = isBot ~ rate, family = "binomial", data = training_data)

            Coefficients:
                    Estimate Std. Error z value Pr(>|z|)
            (Intercept) 0.003402   0.098073   0.035    0.972
            rate        0.214539   0.202490   1.060    0.289

            (Dispersion parameter for binomial family taken to be 1)

            Null deviance: 775.87  on 559  degrees of freedom
            Residual deviance: 774.61  on 558  degrees of freedom
            AIC: 778.61

            Number of Fisher Scoring iterations: 4

        </pre>

        <pre>
            Call:
            glm(formula = isBot ~ rate + sd_inter_response, family = "binomial", 
                data = training_data)

            Coefficients:
                   Estimate Std. Error z value Pr(>|z|)    
            (Intercept)     18.8576     1.6917  11.147   <2e-16 ***
            rate            -7.9457     0.9557  -8.314   <2e-16 ***
            sd_inter_response  -1.6746     0.1563 -10.711   <2e-16 ***
            ---
            Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

            (Dispersion parameter for binomial family taken to be 1)

            Null deviance: 775.87  on 559  degrees of freedom
            Residual deviance: 245.69  on 557  degrees of freedom
            AIC: 251.69

            Number of Fisher Scoring iterations: 7

        </pre>

            <p>
                In the first model, <em>rate</em> is not statistically significant (p = 0.289). However, when <em>sd_inter_response</em> is included 
                as a second predictor, both <em>rate</em> and <em>sd_inter_response</em> become highly significant (p &lt; 0.001). Additionally, the 
                second model's AIC (251.69) is substantially lower than the first model's (778.61), indicating a much better fit. This suggests 
                that the combination of response rate and variability in response timing provides stronger predictive power than response rate alone.

                Thus, our final model is:
            </p>

                <div class="eq-c">
                  <p>\[
                    \log\left(\frac{p}{1 - p}\right) \approx -7.95\cdot\text{rate} - 1.67\cdot\text{sd_inter_response}
                \]</p>
                  </div>

               That  is, the logit score for a new cases is \(7.95\) their response <code>rate</code> plus \(-1.67\) times their response standard deviation 
                (<code>sd_inter_response</code>).
                This can be re-written in terms of probability as see above:
                <p>\[p = \frac{e^{-7.95\cdot\text{rate} - 1.67\cdot\text{sd_inter_response}}}{1 + e^{-7.95\cdot\text{rate} - 1.67\cdot\text{sd_inter_response}}}\]</p>
        <h2>ROC Curve</h2>

            <p>The AUC was found to be 0.94</p>
            <p style="text-align: center;">
                <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/roc_curve.png" alt="ROC Curve" width="600">
            </p>
        <h2>Decision Table</h2>

            <p>
                The decision table below displays all predictive and ROC-based values across a range of cut-points. You may notice that the positive or negative predictive 
                value (PPV or NPV) appears as NaN (Not a Number) at extreme cut-points of 0 and 1. This is not an error—it’s a mathematically valid result.
            </p>
            <p>
                For example, at a cut-point of 0 (i.e., classifying all cases as bots), no cases are predicted as human. The negative predictive value (NPV) 
                is defined as the proportion of predicted negative (humans) that are actually negative. When we place all cases into the positive class 
                (bot class), there are no cases left for asssessment: the NVP calculation considers the number of cases predicted as human. At a cutpoint of zero, 
                no cases were predicted (classified) as human—they are are classified as bots—and we are essentially dividing my zero: resulting in an undefined value (NaN) 

                The same logic applies at a cut-point of 1, where all cases are classified as human. In this case, PPV becomes undefined because there are no predicted positives 
                (bots), leaving the denominator in the PPV formula equal to zero.
                
            </p>
            <p style="text-align: center;">
             <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/decision_table_gt.png" alt="ROC Curve" width="600"
                 width="400" 
                 height="500">
            </p>

            <p>
                Once a cutpoint is selected, a confusion matrix can be constructed. A confusion matrix is a two by two table of the outcomes:
                predicted outcomes and actual outcomes. The outcomes are with respect to what our model was predicting, in this case, human or
                bot response behavior. In each cell of the 2x2 table is a count: the true positive and true negative are on the diagnol and false negatives
                and false positives on the other corners.
            </p>
<table align="center" border="1" cellpadding="8" cellspacing="0">
  <tr>
    <th></th>
    <th>Actual: Human</th>
    <th>Actual: Bot</th>
  </tr>
  <tr>
    <th>Predicted: Human</th>
    <td>True Positive</td>
    <td>False Negative</td>
  </tr>
  <tr>
    <th>Predicted: Bot</th>
    <td>False Positive</td>
    <td>True Negative</td>
  </tr>
</table>

</p>

        <p>
            Accuracy is the total number of correct classifications (true positives + true negatives) divided by the total number of cases.

            </p>
        <div class="eq-c">
          <p>\[
          Accuracy = \frac{tp + tn}{n}
          \]</p>
        </div>

        From the table below, we can verify what is in the decision table above. 
    
       <div class="eq-c">
          <p>\[
          Accuracy = \frac{110 + 109}{240}\\
                     = 0.9125
          \]</p>
        </div>

        The positive predictive value is computed by dividing the number of correct classifications by the number of cases the model predicted as bots:
     <div class="eq-c">
          <p>\[
          PPV = \frac{tp + tn}{129}
                     = 0.8527
          \]</p>
        </div>
                
        <p style="text-align: center;">
             <img src="https://raw.githubusercontent.com/mendoza-cm/Human-vs-Bot/main/visualizations/confusionMatrix.png" alt="Confusion Matrix" width="600"
                 width="100" 
                 height="250">
         </p>

    </section>
    <p>
        The negative predictive value, false positive, and false positive rates can also be calculated from the confusion trable.
    </p>

</body>
</html>
