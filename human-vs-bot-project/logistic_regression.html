<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Logistic Regression</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
            max-width: 900px;
            margin: 0 auto;
        }
        h1 {
            color: #8f0e0a;
            font-size: 2rem;
        }
        h2 {
            color: #c7408d;
            font-size: 1.4rem;
        }
        h3 {
            font-size: 1.2rem;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            list-style: none;
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        .fraction {
          display: inline-block;
          text-align: center;
          vertical-align: middle;
        }
        .fraction .top {
          display: block;
          border-bottom: 1px solid #000;
          padding: 0 0.2em;
        }
        .fraction .bottom {
          display: block;
          padding: 0 0.2em;
        }
        table, th, td {
          border:1px solid black;
        }
    </style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
    <div style="text-align: left; margin-bottom: 2rem;">
    <h1 style="font-size: 2.5rem; color: #444;">Human-vs-Bot</h1>
  </div>
    <!-- main content omitted for brevity -->

    <section>
        <h1>Limitations</h1>
        <p>
            While logistic regression is interpretable and effective for linearly separable data, it has several limitations that are worth addressing:
        </p>
        <ul>
            <li><strong>Linearity Assumption:</strong> Logistic regression assumes a linear relationship between the input variables and the log-odds of the response. This may not hold true if the data contains complex or nonlinear patterns, in which case more flexible models like tree-based methods might perform better.</li>

            <li><strong>Multicollinearity:</strong> Strong correlation between predictors can destabilize coefficient estimates, making interpretation difficult and potentially reducing model performance. Diagnostic checks or dimensionality reduction techniques (e.g., PCA) may be needed in such cases.</li>

            <li><strong>Overfitting:</strong> Including too many features, especially irrelevant ones, can lead to overfitting. Regularization methods like L1 (Lasso) and L2 (Ridge) penalize large coefficients to help prevent this. However, we did not include regularization in the current model. A future extension of this project could incorporate regularized logistic regression to assess whether prediction improves or model coefficients become more robust.</li>

            <li><strong>Threshold Sensitivity:</strong> While the ROC and AUC provide threshold-independent measures of discrimination, real-world decision-making depends on choosing a cut-point. Model performance can vary substantially depending on the threshold selected, especially in imbalanced datasets.</li>

            <li><strong>Simulated Data:</strong> The data used here are synthetic. While they were generated to reflect plausible response behaviors, real-world data may introduce noise, variability, and distributional characteristics that this model hasnâ€™t accounted for. Applying this model to real data would require recalibration and validation.</li>

            <li><strong>Scalability:</strong> Logistic regression scales well to larger datasets, but with high-dimensional features or extremely large data volumes, specialized tools (e.g., stochastic gradient descent implementations, Spark MLlib) may be needed to ensure efficient computation.</li>
        </ul>
        <p>
            Despite these limitations, logistic regression remains a strong baseline and a valuable tool for understanding model behavior. Enhancing this model with regularization and evaluating on more complex or realistic datasets are natural next steps.
        </p>
    </section>
</body>
</html>
