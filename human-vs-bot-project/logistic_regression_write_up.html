<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Logistic Regression</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 2rem;
            background-color: #f9f9f9;
        }
        h1 {
            color: #8f0e0a;
        }
        h2 {
            color: #c7408d;
        }
        h4 {
            color: #0a0a0a;
        }
        ul {
            list-style: none;
            padding-left: 20px;
        }
        li {
            margin: 1rem 0;
        }
        a {
            color: #575959;
            text-decoration: none;
            font-size: 1.2rem;
        }
        a:hover {
            text-decoration: underline;
        }
        .fraction {
  display: inline-block;
  text-align: center;
  vertical-align: middle;
}
.fraction .top {
  display: block;
  border-bottom: 1px solid #000;
  padding: 0 0.2em;
}
.fraction .bottom {
  display: block;
  padding: 0 0.2em;
}
    </style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
    
       
    
    <section> 
        <h1>Logistic Regression</h1>
             <p>Logistic regression models the log-odds of an event as a linear combination of variables; the odds can be re-written in terms of probabilities <var>p</var> : 
<div class="eq-c">
  <p>\[
\log\left(\frac{p}{1 - p}\right) = X\beta
\]</p>
</div>
        Here, \(X\) is an \(n \times (\nu + 1)\) matrix, where <var>n</var> is the number of observations or cases and with \(\nu\) is the number of predictors (excluding the intercept term).
        We use \(\nu\) to avoid confusion with <var>p</var>, which denotes the predicted probability in logistic regression. In this model, \(\beta\) is a \((\nu + 1) \times 1\) vector of coefficients,
        and \(Y\) is an \(n \times 1 \) vector of binary outcomes. 
     </p>   
<p>
    
</p>
<p>
    Logistic regression is often used as a classification tool. To use logistic regression as a classifier, we must choose a threshold or cut-point to convert predicted probabilities 
    into class labels. A common choice is 0.5, meaning that cases with predicted probabilities greater than 0.5 are classified as positive (e.g., bots), and those with probabilities 
    less than or equal to 0.5 as negative (e.g., humans). In our Human-vs-Bot project, we define bots as the positive class (isBot == 1), so a logit score above 0 (i.r., <var>p</var> > 0.5).</p>
    would indicate a likely bot.

<p>
    Deciding where to place a cut-point is not always easy. In many cases, it is imperative to look at several cut-points and their corresponding discrimination metrics and predictive values.
    Discrimination metrics help us assess a modelâ€™s ability to distinguish between the two classes, independent of any specific cut-point. These include the true positive rate (sensitivity), 
    false positive rate, and the area under the receiver operating characteristic (ROC) curve (AUC). AUC measures the probability that a randomly selected positive case (e.g., bot) is ranked 
    higher than a randomly selected negative case (e.g., human), in terms of predicted probability.

    Predictive performance requires selecting a specific cut-point. The positive predictive value (PPV) is the proportion of predicted positives that are true positives (i.e., predicted bots 
    that actually are bots), while the negative predictive value (NPV) is the proportion of predicted negatives that are truly negative (i.e., predicted humans that are actually humans). These 
    metrics depend on both the cut-point and the underlying class distribution.
</p>
<p>
     A decision table is a really good way to organize and consider viable cut-points.  
</p>
    </section>
    <section>
        <h1>Results</h1>
    </section>
    

</body>
</html>
